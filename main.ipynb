{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bpe_idx': [31373, 612],\n",
       " 'tokens': ['hello', ' there'],\n",
       " 'parts': [{'token': 'hello',\n",
       "   'token_bytes': b'hello',\n",
       "   'token_translated': 'hello',\n",
       "   'token_merged': ['hello'],\n",
       "   'token_ix': [31373]},\n",
       "  {'token': ' there',\n",
       "   'token_bytes': b' there',\n",
       "   'token_translated': 'Ġthere',\n",
       "   'token_merged': ['Ġthere'],\n",
       "   'token_ix': [612]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "import lora\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model\n",
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Trains a small language model that can learn to speak englisch with very few parameters \n",
    "using the TinyStories Dataset.\n",
    "\n",
    "For Fine-Tuning, you can either use full-weights fine-tuning (just resume training and change the dataset\n",
    "location) or use low-rank adapters to greatly reduce the amount of trainable parameters (faster fine-tuning).\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def _lazy_file_read(file_obj, chunk_size=1024):\n",
    "    while True:\n",
    "        data = file_obj.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "\n",
    "def pre_tokenize_dataset(path, save_path):\n",
    "    print(f\"Running tokenization for {path}\")\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            arrays = []\n",
    "            for line in tqdm(_lazy_file_read(file)):\n",
    "                tokens = tokenizer(line).data['input_ids']\n",
    "                arrays.append(np.array(tokens, dtype=np.int32))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        np.save(save_path, np.concatenate(arrays))\n",
    "        print(f\"Saved tokenized file to binary {save_path}\")\n",
    "\n",
    "\n",
    "class TinyStoriesDataset(data.IterableDataset):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, tokenized_path, block_size: int, device: str = 'cuda'):\n",
    "        # Each line represents a short story\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "        self.train = np.load(tokenized_path, mmap_mode='r', allow_pickle=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idx = np.random.randint(0, len(self.train) - self.block_size, 1)[0]\n",
    "            chunk = self.train[idx:idx + self.block_size + 1]\n",
    "            source = torch.tensor(chunk[:-1], device=self.device, dtype=torch.long)\n",
    "            target = torch.tensor(chunk[1:], device=self.device, dtype=torch.long)\n",
    "            yield source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(training_model: torch.nn.Module, val_loader: torch.utils.data.DataLoader):\n",
    "    training_model.eval()\n",
    "    losses = torch.zeros(config['EVAL_ITER'])\n",
    "    for k in range(config['EVAL_ITER']):\n",
    "        s_val, t_val = next(iter(val_loader))\n",
    "        val_logits = training_model(s_val)\n",
    "        val_logits: torch.Tensor = val_logits.view(config['BATCH_SIZE'] * config['BLOCK_SIZE'], config['VOCAB_SIZE'])\n",
    "        t_val = t_val.view(config['BATCH_SIZE'] * config['BLOCK_SIZE'])\n",
    "        losses[k] = torch.nn.functional.cross_entropy(val_logits, t_val).item()\n",
    "    training_model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample_text(training_model: model.TinyLM, max_tokens: int = 200) -> str:\n",
    "    training_model.eval()\n",
    "    context = torch.zeros((5, config['BLOCK_SIZE']), dtype=torch.long, device=config['DEVICE'])\n",
    "    out_tokens = training_model.generate(context, max_new_tokens=max_tokens)\n",
    "    # Reform to one long piece of text\n",
    "    out_tokens = out_tokens.view(out_tokens.shape[0] * out_tokens.shape[1])\n",
    "    training_model.train()\n",
    "    return tokenizer.decode(out_tokens)\n",
    "\n",
    "\n",
    "# ----------------------------------------- SETUP ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "config = {\n",
    "    \"BLOCK_SIZE\": 128,\n",
    "    \"EMB_SIZE\": 768,\n",
    "    \"N_ATTENTION_HEADS\": 4,\n",
    "    \"N_DECODER_BLOCKS\": 2,\n",
    "    \"VOCAB_SIZE\": len(tokenizer),\n",
    "    \"MAX_OUT_TOKENS\": 200,\n",
    "    \"EVAL_INTERVAL\": 1000,\n",
    "    \"EVAL_ITER\": 100,\n",
    "    \"LR\": 3e-4,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"DEVICE\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"LOAD_PATH\": 'models/tiny_base.pt',\n",
    "    \"SAVE_PATH\": 'models/tiny_base_lora.pt',\n",
    "    \"ENABLE_LORA\": True,\n",
    "}\n",
    "assert config['EMB_SIZE'] % config['N_ATTENTION_HEADS'] == 0\n",
    "prev_epochs = 0\n",
    "\n",
    "# Create model instance\n",
    "print(f\"Loading model on device {config['DEVICE']}\")\n",
    "model = model.TinyLM(emb_dim=config['EMB_SIZE'], block_size=config['BLOCK_SIZE'],\n",
    "                     n_att_heads=config['N_ATTENTION_HEADS'], n_decoders=config['N_DECODER_BLOCKS'],\n",
    "                     vocab_size=config['VOCAB_SIZE'], device=config['DEVICE'])\n",
    "\n",
    "model = model.to(config['DEVICE'])\n",
    "\n",
    "lora_enabled_on_base = False\n",
    "checkpoint = None\n",
    "# Load pre-trained base model if it exists\n",
    "if os.path.exists(config['LOAD_PATH']):\n",
    "    checkpoint = torch.load(config['LOAD_PATH'], map_location=config['DEVICE'])\n",
    "    prev_epochs = checkpoint['epoch']\n",
    "    lora_enabled_on_base = checkpoint['lora_was_enabled']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# INJECT LoRA, if enabled AND not already enabled in the base weights\n",
    "# IF lora was in the base weights, there is no need to create the layers again, as they already exist!\n",
    "# --> Training will just continue to train the LoRA layers.\n",
    "should_inject_lora = config['ENABLE_LORA'] and not lora_enabled_on_base\n",
    "if should_inject_lora:\n",
    "    lora.inject_lora(model, [\"self_attention\"], 2, 0.1, config['DEVICE'])\n",
    "\n",
    "# Get trainable parameters\n",
    "trainable_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "total_params = sum(p.numel() for p in trainable_parameters)\n",
    "print(f\"\\nTotal trainable parameters: {total_params}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optim = torch.optim.Adam(trainable_parameters, lr=config['LR'])\n",
    "# Load the state of the optimizer only if training is continued with the same structure.\n",
    "if checkpoint and not should_inject_lora:\n",
    "    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "TINY_STORY_TRAIN = 'data/TinyStories-train.txt'\n",
    "TINY_TOKENIZED = 'data/tiny_tokenized.npy'\n",
    "\n",
    "if not os.path.exists(TINY_TOKENIZED):\n",
    "    pre_tokenize_dataset(TINY_STORY_TRAIN, TINY_TOKENIZED)\n",
    "\n",
    "train_tiny_stories = TinyStoriesDataset(TINY_TOKENIZED, config['BLOCK_SIZE'], device=config['DEVICE'])\n",
    "train_loader = data.DataLoader(train_tiny_stories, batch_size=config['BATCH_SIZE'])\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "TINY_STORY_VAL = 'data/TinyStories-valid.txt'\n",
    "TINY_TOKENIZED_VAL = 'data/tiny_tokenized_val.npy'\n",
    "\n",
    "if not os.path.exists(TINY_TOKENIZED_VAL):\n",
    "    pre_tokenize_dataset(TINY_STORY_VAL, TINY_TOKENIZED_VAL)\n",
    "\n",
    "val_tiny_stories = TinyStoriesDataset(TINY_TOKENIZED_VAL, config['BLOCK_SIZE'], device=config['DEVICE'])\n",
    "val_loader = data.DataLoader(val_tiny_stories, batch_size=config['BATCH_SIZE'])\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------- TRAINING -------------------------------------------------------------------\n",
    "\n",
    "# SETUP Wands & Biases for eval logging\n",
    "wandb.init(\n",
    "    project='TinyLM',\n",
    "    config=config\n",
    ")\n",
    "text_table = wandb.Table(columns=['epoch', 'loss', 'predicted text'])\n",
    "\n",
    "try:\n",
    "    for b_idx, batch in enumerate(train_loader):\n",
    "        # Inference\n",
    "        sources, targets = batch\n",
    "        logits = model(sources)\n",
    "        logits = logits.view(config['BATCH_SIZE'] * config['BLOCK_SIZE'], config['VOCAB_SIZE'])\n",
    "        targets = targets.view(config['BATCH_SIZE'] * config['BLOCK_SIZE'])\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        wandb.log({\"loss\": loss})\n",
    "        # Weight update\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if b_idx % config['EVAL_INTERVAL'] == 0:\n",
    "            val_loss = eval_model(model, val_loader)\n",
    "            generated_text = generate_sample_text(model, max_tokens=config['MAX_OUT_TOKENS'])\n",
    "            print(generated_text)\n",
    "            wandb.log({\"val_loss\": val_loss})\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    checkpoint_location = config['LOAD_PATH']\n",
    "    if config['SAVE_PATH']:\n",
    "        checkpoint_location = config['SAVE_PATH']\n",
    "\n",
    "    print(f\"Saving model to {checkpoint_location} and shutting down training...\")\n",
    "    torch.save({'epoch': prev_epochs + b_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict(),\n",
    "                'lora_was_enabled': config['ENABLE_LORA'],\n",
    "                'config': config,\n",
    "                }, checkpoint_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
